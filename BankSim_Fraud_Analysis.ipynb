{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 1) Import & Load\n",
        "# ================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.ticker import PercentFormatter\n",
        "from sklearn.metrics import precision_score, recall_score, confusion_matrix\n",
        "from pathlib import Path\n",
        "\n",
        "df = pd.read_csv(r\"C:\\Users\\beras\\Desktop\\Fraud_analyst_project\\bs140513_032310.csv\")\n",
        "print(\"Loaded file:\", df)\n"
      ],
      "metadata": {
        "id": "NQYxyl0MMMVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 2) Quick examination\n",
        "# ================================\n",
        "print(\"Dataset shape (rows, columns):\", df.shape)\n",
        "print(df.head(10))\n",
        "\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"\\nMissing values per column:\\n\", missing_values)\n",
        "\n",
        "duplicate_rows = int(df.duplicated().sum())\n",
        "print(f\"\\nNumber of duplicate rows: {duplicate_rows}\")\n",
        "\n",
        "print(\"\\nData types:\\n\", df.dtypes)\n",
        "\n",
        "print(\"\\nNumber of unique values per column:\\n\", df.nunique(dropna=False).sort_values())"
      ],
      "metadata": {
        "id": "sWVKYe3mMQJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Block 3 — Minimal cleaning + gender normalization\n",
        "# ============================================\n",
        "\n",
        "# drop ZIP columns at the start of the block (both are constant)\n",
        "df = df.drop(columns=['zipcodeOri', 'zipMerchant'])\n",
        "print(\"Dropped constant ZIP columns: zipcodeOri, zipMerchant\")\n",
        "\n",
        "# 1) strip quotes/whitespace in relevant text columns\n",
        "text_cols = ['customer','age','gender','merchant','category']\n",
        "for c in text_cols:\n",
        "    df[c] = (\n",
        "        df[c].astype(str)\n",
        "             .str.strip()\n",
        "             .str.strip(\"'\")\n",
        "             .str.strip('\"')\n",
        "    )\n",
        "\n",
        "# 2) normalize categories\n",
        "df['gender']   = df['gender'].str.lower().replace({'e':'unknown', 'u':'unknown'})\n",
        "df.loc[~df['gender'].isin(['m','f','unknown']), 'gender'] = 'unknown'\n",
        "df['category'] = df['category'].str.lower()\n",
        "\n",
        "# 3) convert 'age' -> numeric (nullable Int64) — may create NaN/<NA> for non-numeric\n",
        "df['age'] = pd.to_numeric(df['age'], errors='coerce').astype('Int64')\n",
        "\n",
        "# 4) derive temporal features from 'step'\n",
        "df['day']  = (df['step'] // 24).astype(int)\n",
        "df['hour'] = (df['step'] % 24).astype(int)\n",
        "\n",
        "# --- sanity check\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"Missing values per column:\")\n",
        "print(df.isna().sum().sort_values(ascending=False))"
      ],
      "metadata": {
        "id": "NSrdvEjNMSyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Block 3B — Fill NaNs discovered + sentinel\n",
        "# ============================================\n",
        "\n",
        "# create a flag (1 if age was missing)\n",
        "df['age_unknown'] = df['age'].isna().astype('int64')\n",
        "\n",
        "# fill 'age' with sentinel -1 (keep Int64)\n",
        "df['age'] = df['age'].fillna(-1).astype('Int64')\n",
        "\n",
        "# quick sanity check after fill\n",
        "print(\"\\nNaN in 'age' AFTER fill:\", int(df['age'].isna().sum()))"
      ],
      "metadata": {
        "id": "q6f0KQijMWr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 4) EDA\n",
        "# ================================\n",
        "def pct(x):\n",
        "    return (x * 100).round(3)\n",
        "\n",
        "# Class balance\n",
        "class_balance = pd.concat(\n",
        "    [df['fraud'].value_counts().rename('count'),\n",
        "     pct(df['fraud'].value_counts(normalize=True)).rename('percent')],\n",
        "    axis=1\n",
        ").rename_axis('fraud').reset_index()\n",
        "print(\"\\nClass balance:\\n\", class_balance)\n",
        "\n",
        "plt.figure(figsize=(4,3))\n",
        "sns.barplot(data=class_balance, x='fraud', y='count')\n",
        "plt.title('Class balance')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---- global plotting params (single source of truth) ----\n",
        "CATEGORICAL_FIGSIZE = (12, 5)\n",
        "NUMERIC_FIGSIZE     = (6, 4)\n",
        "TOP_N               = 20\n",
        "\n",
        "# columns to plot\n",
        "categorical_columns = ['category', 'merchant', 'gender', 'day', 'hour', 'step']\n",
        "numeric_columns     = ['amount', 'age']\n",
        "\n",
        "# convert day/hour to string for categorical plotting consistency\n",
        "df['day']  = df['day'].astype(str)\n",
        "df['hour'] = df['hour'].astype(str)\n",
        "\n",
        "\n",
        "# ---------- plotting: percent within category (0..100), safe to run always ----------\n",
        "def plot_categorical_distribution_percent(df, column):\n",
        "    \"\"\"\n",
        "    For each value in `column`, compute percent of fraud vs non-fraud relative to that category's total,\n",
        "    then plot percent bars (0-100). Uses TOP_N top values by count for readability.\n",
        "    \"\"\"\n",
        "    order = df[column].astype(str).value_counts().index[:TOP_N]\n",
        "    sub = df[df[column].astype(str).isin(order)].copy()\n",
        "    sub[column] = sub[column].astype(str)\n",
        "\n",
        "    # pivot: counts per (category value, fraud)\n",
        "    pivot = sub.groupby([column, 'fraud']).size().unstack(fill_value=0)\n",
        "\n",
        "    # ensure both columns 0 and 1 exist (robustness)\n",
        "    for c in [0, 1]:\n",
        "        if c not in pivot.columns:\n",
        "            pivot[c] = 0\n",
        "\n",
        "    # percent within category (rows sum to 100)\n",
        "    pivot_percent = pivot.div(pivot.sum(axis=1), axis=0).multiply(100)\n",
        "    pivot_percent = pivot_percent.reset_index().melt(id_vars=column, value_vars=[0,1],\n",
        "                                                     var_name='fraud', value_name='percent')\n",
        "    if column == 'merchant' or column == 'category':\n",
        "        XTICKS_ROTATION = 90\n",
        "    else:\n",
        "        XTICKS_ROTATION = 0\n",
        "\n",
        "    plt.figure(figsize=CATEGORICAL_FIGSIZE)\n",
        "    sns.barplot(data=pivot_percent, x=column, y='percent', hue='fraud', order=order)\n",
        "    plt.gca().yaxis.set_major_formatter(PercentFormatter(xmax=100))\n",
        "    handles, lbls = plt.gca().get_legend_handles_labels()\n",
        "    plt.legend(handles=handles, labels=[f\"fraud = {l}\" for l in lbls], title='')\n",
        "    plt.title(f\"{column} — percent by fraud within each category (top {len(order)})\")\n",
        "    plt.xlabel(column)\n",
        "    plt.ylabel(\"Percent (within category)\")\n",
        "    plt.xticks(rotation=XTICKS_ROTATION, ha='right')\n",
        "    plt.ylim(0, 100)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_numeric_by_fraud(df, column):\n",
        "    \"\"\"\n",
        "    Simple numeric boxplot grouped by fraud.\n",
        "    The caller should decide which numeric columns to pass (e.g., 'amount_for_plot' for log10(amount)).\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=NUMERIC_FIGSIZE)\n",
        "    sns.boxplot(data=df, x='fraud', y=column)\n",
        "    plt.title(f\"{column} by fraud label\")\n",
        "    plt.xlabel(\"fraud\")\n",
        "    plt.ylabel(column)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def summary_categorical_table(df, column):\n",
        "    \"\"\"\n",
        "    Summary table for a categorical column (only categories with n_fraud > 0).\n",
        "    Returns columns: category_value, n_total, n_fraud, fraud_rate_pct, share_of_all_frauds_pct.\n",
        "    \"\"\"\n",
        "    total_frauds = df['fraud'].sum()\n",
        "\n",
        "    summary_df = (\n",
        "        df.groupby(column)['fraud']\n",
        "          .agg(n_total='count', n_fraud='sum')\n",
        "          .reset_index()\n",
        "    )\n",
        "\n",
        "    # keep only categories that actually contain frauds\n",
        "    summary_df = summary_df[summary_df['n_fraud'] > 0]\n",
        "\n",
        "    summary_df['fraud_rate_pct'] = (summary_df['n_fraud'] / summary_df['n_total'] * 100).round(3)\n",
        "    summary_df['share_of_all_frauds_pct'] = (\n",
        "        (summary_df['n_fraud'] / total_frauds * 100)\n",
        "        .replace([np.inf, np.nan], 0)\n",
        "        .round(3)\n",
        "    )\n",
        "\n",
        "    summary_df = summary_df.sort_values('fraud_rate_pct', ascending=False).rename(columns={column: 'category_value'}).reset_index(drop=True)\n",
        "    return summary_df\n",
        "\n",
        "\n",
        "def summary_numeric_binned_table(df, column):\n",
        "    \"\"\"\n",
        "    Bin numeric column into `bins` and return summary table (only bins with n_fraud > 0).\n",
        "    Returns columns: value_bin, n_total, n_fraud, fraud_rate_pct, share_of_all_frauds_pct.\n",
        "    \"\"\"\n",
        "    value_bin = pd.qcut(df[column], q=4)\n",
        "    total_frauds = df['fraud'].sum()\n",
        "\n",
        "    summary_df = (\n",
        "        df.groupby(value_bin)['fraud']\n",
        "          .agg(n_total='count', n_fraud='sum')\n",
        "          .reset_index()\n",
        "          .rename(columns={'index': 'value_bin'})\n",
        "    )\n",
        "\n",
        "    # keep only bins that contain >=1 fraud\n",
        "    summary_df = summary_df[summary_df['n_fraud'] > 0]\n",
        "\n",
        "    summary_df['fraud_rate_pct'] = (summary_df['n_fraud'] / summary_df['n_total'] * 100).round(3)\n",
        "    summary_df['share_of_all_frauds_pct'] = (\n",
        "        (summary_df['n_fraud'] / total_frauds * 100)\n",
        "        .replace([np.inf, np.nan], 0)\n",
        "        .round(3)\n",
        "    )\n",
        "\n",
        "    summary_df = summary_df.sort_values('fraud_rate_pct', ascending=False).reset_index(drop=True)\n",
        "    return summary_df\n",
        "\n",
        "\n",
        "# Run categorical plots\n",
        "print(\"\\nCategorical percent distributions by fraud (within-category %):\")\n",
        "for col in categorical_columns:\n",
        "    plot_categorical_distribution_percent(df, col)\n",
        "\n",
        "# Run numeric plots\n",
        "print(\"\\nNumeric distributions by fraud:\")\n",
        "for col in numeric_columns:\n",
        "    plot_numeric_by_fraud(df, col)\n",
        "\n",
        "# --- Run categorical summaries for each categorical column ---\n",
        "print(\"\\n=== CATEGORICAL SUMMARIES (top rows) ===\")\n",
        "for col in categorical_columns:\n",
        "    print(f\"\\n--- Summary for: {col} ---\")\n",
        "    cat_summary = summary_categorical_table(df, col)   # returns DataFrame (only categories with n_fraud > 0)\n",
        "    print(cat_summary.head(20))                      # show top-20 rows\n",
        "\n",
        "# --- Run numeric binned summaries for each numeric column ---\n",
        "print(\"\\n=== NUMERIC BINNED SUMMARIES (top rows) ===\")\n",
        "for col in numeric_columns:\n",
        "    print(f\"\\n--- Binned summary for: {col} ---\")\n",
        "    num_summary = summary_numeric_binned_table(df, col)\n",
        "    print(num_summary.head(20))"
      ],
      "metadata": {
        "id": "5GF3TYHHMaNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 5) Feature “alerts”\n",
        "# ================================\n",
        "label_column = 'fraud'\n",
        "global_fraud_rate = df[label_column].mean()\n",
        "print(f\"\\nGlobal fraud rate: {global_fraud_rate*100:.3f}%\")\n",
        "\n",
        "\n",
        "category_stats = df.groupby('category')[label_column].agg(count='count', frauds='sum')\n",
        "category_stats['rate'] = category_stats['frauds']/category_stats['count']\n",
        "category_stats['lift'] = category_stats['rate']/global_fraud_rate\n",
        "print(category_stats['lift'])\n",
        "\n",
        "\n",
        "merchant_stats = df.groupby('merchant')[label_column].agg(count='count', frauds='sum')\n",
        "merchant_stats['rate'] = merchant_stats['frauds']/merchant_stats['count']\n",
        "merchant_stats['lift'] = merchant_stats['rate']/global_fraud_rate\n",
        "print(merchant_stats['lift'])\n",
        "\n",
        "\n",
        "# risky sets by lift + minimum support\n",
        "min_support = 200\n",
        "min_lift = 2.0\n",
        "\n",
        "high_lift_categories = category_stats.query(\"count >= @min_support and lift >= @min_lift\").index.tolist()\n",
        "\n",
        "high_lift_merchants = merchant_stats.query(\"count >= @min_support and lift >= @min_lift\").index.tolist()\n",
        "\n",
        "print(f\"Risky sets — categories: {len(high_lift_categories)} | merchants: {len(high_lift_merchants)}\")\n",
        "\n",
        "# amount thresholds\n",
        "amount_q99_global = df['amount'].quantile(0.99)\n",
        "category_amount_q95    = df.groupby('category')['amount'].quantile(0.95)\n",
        "category_amount_q90    = df.groupby('category')['amount'].quantile(0.90)\n",
        "\n",
        "# chronological order + behavioral features (burst, first-time)\n",
        "df = df.sort_values(['customer','step']).reset_index(drop=True)\n",
        "df['ft_mer'] = df.groupby(['customer','merchant']).cumcount() == 0\n",
        "\n",
        "cust_step = df.groupby(['customer','step']).size().rename('cust_step_n').reset_index()\n",
        "cust_step = cust_step.sort_values(['customer','step']).reset_index(drop=True)\n",
        "cust_step['cust_prev6_n'] = (\n",
        "    cust_step.groupby('customer')['cust_step_n']\n",
        "             .transform(lambda s: s.shift().rolling(6, min_periods=1).sum())\n",
        ").fillna(0).astype(int)\n",
        "df = df.merge(cust_step, on=['customer','step'], how='left')\n",
        "\n",
        "# deviation from customer's median amount\n",
        "tiny_eps = 1e-9\n",
        "df['cust_median_amount'] = df.groupby('customer')['amount'].transform('median').fillna(0)\n",
        "df['amt_over_cust_median'] = df['amount'] / (df['cust_median_amount'] + tiny_eps)\n",
        "amount_deviation_threshold = max(df['amt_over_cust_median'].quantile(0.99), 5.0)\n",
        "\n",
        "# vectorized masks\n",
        "mask_amount_above_global_q99 = df['amount'] > amount_q99_global\n",
        "mask_amount_above_category_q95    = df['amount'] > df['category'].map(category_amount_q95).fillna(np.inf)\n",
        "mask_amount_above_category_q90    = df['amount'] > df['category'].map(category_amount_q90).fillna(np.inf)\n",
        "\n",
        "mask_high_lift_category  = df['category'].isin(high_lift_categories)\n",
        "mask_high_lift_merchant  = df['merchant'].isin(high_lift_merchants)\n",
        "\n",
        "mask_first_time_merchant_for_customer  = df['ft_mer']\n",
        "mask_amount_deviates_from_customer_median  = df['amt_over_cust_median'] > amount_deviation_threshold\n",
        "\n",
        "# burst (6 steps) - threshold by 0.99 quantile\n",
        "burst_quantile = 0.99\n",
        "burst_threshold_count = int(max(2, np.nanquantile(df['cust_prev6_n'], burst_quantile)))\n",
        "mask_customer_burst  = df['cust_prev6_n'] >= burst_threshold_count\n",
        "print(f\"[Burst] threshold: >= {burst_threshold_count} (q={burst_quantile})\")\n",
        "\n",
        "# deterministic rules\n",
        "df['rule_amount_grtr_global_q99']     = mask_amount_above_global_q99\n",
        "df['rule_amount_grtr_category_q95'] = mask_amount_above_category_q95\n",
        "df['rule_high_lift_category']    = mask_high_lift_category\n",
        "df['rule_high_lift_merchant']    = mask_high_lift_merchant\n",
        "df['rule_customer_burst6']       = mask_customer_burst\n",
        "df['rule_age_missing']       = (df['age_unknown'] == 1)\n",
        "\n",
        "# AND/OR rules\n",
        "df['rule_high_merchant_high_amount'] = mask_high_lift_merchant & (df['amount'] >= 50)\n",
        "df['rule_high_category_and_amount'] = mask_high_lift_category & (df['amount'] >= 42.54)\n",
        "df['rule_high_cat_or_mer_and_amount'] = (mask_high_lift_category | mask_high_lift_merchant) & (df['amount'] >= 55)\n",
        "df['rule_high_risk'] = (mask_high_lift_category | mask_high_lift_merchant) & (mask_amount_above_category_q90 | mask_amount_above_global_q99 | mask_first_time_merchant_for_customer | mask_customer_burst | mask_amount_deviates_from_customer_median)\n",
        "df['rule_high_lift_amount_behavior'] = ((mask_high_lift_category | mask_high_lift_merchant) & (mask_amount_above_category_q95 | mask_amount_above_global_q99) & (mask_first_time_merchant_for_customer | mask_customer_burst | mask_amount_deviates_from_customer_median))\n"
      ],
      "metadata": {
        "id": "OdYd1pLaMtOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 6) Evaluate rules (precision / recall / lift) using sklearn\n",
        "# ================================\n",
        "\n",
        "# label column (set explicitly for reproducibility)\n",
        "label_column = 'fraud'\n",
        "\n",
        "y_true = df[label_column].astype(int).to_numpy()\n",
        "global_fraud_rate = y_true.mean() if len(y_true) else 0.0\n",
        "\n",
        "def eval_rule_sklearn(y_true, y_pred_series, name):\n",
        "    \"\"\"\n",
        "    y_true: numpy array of 0/1\n",
        "    y_pred_series: pandas Series boolean/int\n",
        "    returns dict with rule, flagged, tp, precision, recall, lift\n",
        "    \"\"\"\n",
        "    y_pred = y_pred_series.astype(int).to_numpy()\n",
        "    flagged = int(y_pred.sum())\n",
        "    # use zero_division=0 to avoid exceptions when no positive predictions\n",
        "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
        "    lift = (precision / global_fraud_rate) if global_fraud_rate > 0 else np.nan\n",
        "    return {\n",
        "        'rule': name,\n",
        "        'flagged': flagged,\n",
        "        'tp': int(tp),\n",
        "        'precision': round(precision, 4),\n",
        "        'recall': round(recall, 4),\n",
        "        'lift': round(lift, 2)\n",
        "    }\n",
        "\n",
        "# list of rule columns to evaluate\n",
        "rule_cols = [\n",
        "    'rule_amount_grtr_global_q99',\n",
        "    'rule_amount_grtr_category_q95',\n",
        "    'rule_high_lift_category',\n",
        "    'rule_high_lift_merchant',\n",
        "    'rule_customer_burst6',\n",
        "    'rule_age_missing',\n",
        "    'rule_high_merchant_high_amount',\n",
        "    'rule_high_category_and_amount',\n",
        "    'rule_high_cat_or_mer_and_amount',\n",
        "    'rule_high_risk',\n",
        "    'rule_high_lift_amount_behavior'\n",
        "]\n",
        "\n",
        "results = []\n",
        "for rc in rule_cols:\n",
        "    results.append(eval_rule_sklearn(y_true, df[rc], rc))\n",
        "\n",
        "eval_tbl = pd.DataFrame(results).sort_values(['recall', 'precision'], ascending=[False, False]).reset_index(drop=True)\n",
        "print(\"\\nRule evaluation (sorted by recall, then precision):\")\n",
        "print(eval_tbl)\n",
        "\n",
        "eval_tbl['score'] = eval_tbl['recall'] * eval_tbl['precision']\n",
        "top5_rules = eval_tbl.sort_values('score', ascending=False).head(5)\n",
        "\n",
        "print(\"\\n=== Top 5 Rules by recall*precision score ===\")\n",
        "print(top5_rules)"
      ],
      "metadata": {
        "id": "xu-H_VoNMnyg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}